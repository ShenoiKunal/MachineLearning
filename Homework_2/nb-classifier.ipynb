{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyMbEQSQoRI0Hn/QAyDQxC6U"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# PA2 - NB Classification\n",
    "\n",
    "In this homework assignment, you will implement a simple Naive Bayes classifier and train/test it on the 20 newsgroup dataset. The objective is to classify documents into one of the 20 categories using a simple Naive Bayesian method.\n",
    "\n",
    "The following code is partially done. You need to complete all the code blocks that start with **TODO**. Save a copy of this notebook, and complete your notebook. **Submit your completed notebook with all the outputs returned and displayed.**\n",
    "\n",
    "## Data Analysis\n",
    "Let's fetch the dataset first. You can download it using a sci-kit learn dataset method as below."
   ],
   "metadata": {
    "id": "IFDpOc0Imaj4"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aTSlqjGlmE9a"
   },
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "data_tr = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's inspect what we have."
   ],
   "metadata": {
    "id": "-HtbYeI2_Lre"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dir(data_tr)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r-XZS21ons5c",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1675376111858,
     "user_tz": 300,
     "elapsed": 110,
     "user": {
      "displayName": "Jiho Noh",
      "userId": "12416051139264536919"
     }
    },
    "outputId": "ca7fd948-6943-4c3f-b954-5484dbfe7312"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's print out the first three examples."
   ],
   "metadata": {
    "id": "P-VCxw4Y_vJ6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "data_tr.data[:3]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lGZvIq9ZnwZ7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1675376158652,
     "user_tz": 300,
     "elapsed": 194,
     "user": {
      "displayName": "Jiho Noh",
      "userId": "12416051139264536919"
     }
    },
    "outputId": "5c044ec3-2fcd-4c1e-fb3a-09b39b4dc36a"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "data_tr.target[:3]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cl-oacrDoTVb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1675376189409,
     "user_tz": 300,
     "elapsed": 135,
     "user": {
      "displayName": "Jiho Noh",
      "userId": "12416051139264536919"
     }
    },
    "outputId": "8b19e10d-e486-4b3f-bdc6-6d575b8f9c80"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, `data` is the textual input, and `target` is the corresponding label (i.e., news category). We have 20 classes as below, and the target is the integer number corresponding to the names:"
   ],
   "metadata": {
    "id": "R5l0oUqw_26T"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "data_tr.target_names"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s_5fvg3x_jc_",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1675376209296,
     "user_tz": 300,
     "elapsed": 168,
     "user": {
      "displayName": "Jiho Noh",
      "userId": "12416051139264536919"
     }
    },
    "outputId": "3bdfd98a-a9a5-4ab3-aa16-b7bbb88fec9c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's plot a histogram with the target data to see its distribution."
   ],
   "metadata": {
    "id": "rDrCrwNIAkRw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(data_tr.target, bins=len(data_tr.target_names))\n",
    "plt.title(\"20 Newsgroup category frequence\")\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "4Y3vlcB9o32b",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1675376281818,
     "user_tz": 300,
     "elapsed": 669,
     "user": {
      "displayName": "Jiho Noh",
      "userId": "12416051139264536919"
     }
    },
    "outputId": "b0042592-af77-4385-9d75-13812e96158e"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Text preprocessing\n",
    "\n",
    "From this section, you will complete the unfinished code to make it run as expected. First part is for text preprocessing.\n",
    "\n",
    "### Filtering of Stopwords and Tokenizing\n",
    "\n",
    "You will remove stopwords from the raw texts. *Stopwords* are a set of frequently used words in any language, such as 'a', 'you', 'I', and 'will'. These are most likely function words that are not necessarily informative in delivering meanings.\n",
    "\n",
    "Next, the given texts will be tokenized; that is, a sentence is split into an array of words by whitespace. After that, we can compute the probabilities over words.\n",
    "\n",
    "So, this is a simple text preprocessing that should:\n",
    "- convert to lowercase\n",
    "- filter stopwords, and\n",
    "- tokenize simply by whitespace.\n",
    "\n",
    "We can use the NLTK stopwords as below:"
   ],
   "metadata": {
    "id": "OdbEFMfuibO2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "sw_list = stopwords.words('english')\n",
    "sw_list[:10]  # show some examples"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lvq6u-HPl0kX",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1675376440013,
     "user_tz": 300,
     "elapsed": 936,
     "user": {
      "displayName": "Jiho Noh",
      "userId": "12416051139264536919"
     }
    },
    "outputId": "35136cd6-71ae-4b1d-a885-7f14174b9390"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**TODO -- Complete the following function; this function takes a string and returns a list of words in lowercase which are not in the stopwords list.**"
   ],
   "metadata": {
    "id": "TG76aobqCk2o"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def text_prep(s):\n",
    "  # TODO -- write your code here\n",
    "  # remove stopwords\n",
    "  # lowercase\n",
    "  # tokenize\n",
    "  output = []\n",
    "  for word in s.split():\n",
    "    if word in sw_list:\n",
    "      continue\n",
    "    output.append(str.lower(word))\n",
    "  return output\n"
   ],
   "metadata": {
    "id": "dss49MPkCYUY"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**TODO -- Apply the function `text_prep` on `data_tr.data` and append the outputs to `data_tr_prep`. `data_tr_prep` will be a list of documents where each document is a list of preprocessed words.**"
   ],
   "metadata": {
    "id": "ITqVT5HqDhTe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "data_tr_prep = []\n",
    "# TODO -- write your code here\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "shgQs37BlR2k",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1674535606250,
     "user_tz": 300,
     "elapsed": 7200,
     "user": {
      "displayName": "Jiho Noh",
      "userId": "12416051139264536919"
     }
    },
    "outputId": "39d6dab6-d29e-447d-c214-c489a830385d"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "`data_tr_prep[0]` should print a list of words similar to the following list:\n",
    "```\n",
    "['from:',\n",
    " 'lerxst@wam.umd.edu',\n",
    " \"(where's\",\n",
    " 'thing)',\n",
    " 'subject:',\n",
    " 'car',\n",
    " 'this!?',\n",
    " 'nntp-posting-host:',\n",
    " 'rac3.wam.umd.edu',\n",
    " 'organization:',\n",
    " 'university',\n",
    " 'maryland,',\n",
    " ...\n",
    "]\n",
    "```"
   ],
   "metadata": {
    "id": "ipz6qv95E_14"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build a Vocabulary\n",
    "\n",
    "Now we need to build a vocabulary which contains a fixed number of unique words. Only the words in the vocabulary will be used in the prediction process.\n",
    "\n",
    "Let's set a reasonable size of vocabulary (i.e., V = 10000)\n",
    "\n",
    "We will use a Python Counter to count all the words appear in the entire *training* dataset. This counter is a dictionary of key-value (word-frequency) pairs.\n"
   ],
   "metadata": {
    "id": "vcYR1VEGqNXm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from collections import Counter, defaultdict\n",
    "V = 10000\n",
    "C = len(data_tr.target_names)\n",
    "cnt_words = Counter()\n",
    "for d in data_tr_prep:\n",
    "  cnt_words.update(d)"
   ],
   "metadata": {
    "id": "5YCX21cGpycC"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "`.most_common(n)` will return the n most frequent words in the counter, as below: Let's not worry about the punctuation words for now."
   ],
   "metadata": {
    "id": "A7y8SEQGHO0f"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "cnt_words.most_common(20)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7-J1K1-nIBSP",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1674536124390,
     "user_tz": 300,
     "elapsed": 328,
     "user": {
      "displayName": "Jiho Noh",
      "userId": "12416051139264536919"
     }
    },
    "outputId": "e6ce2987-106d-473d-dfb7-ad1d269a1313"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**TODO -- Build mappings between tokens (words) and their index numbers.**\n",
    "\n",
    "We create a data structure for the vocabulary of `V` words. You can use `cnt_words.most_common(V)` to get the top V most frequent words.\n",
    "\n",
    "`tok2idx` should map a token to its index number and `idx2tok` should be a list of words in the frequency order."
   ],
   "metadata": {
    "id": "y2ravLsMIdGm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "idx2tok = list()\n",
    "tok2idx = dict()\n",
    "\n",
    "# TODO -- write your code here to populate the idx2tok and tok2idx\n",
    "\n"
   ],
   "metadata": {
    "id": "_36wcYG1q7mh"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "You should see results like below:\n",
    "\n",
    "```\n",
    "> idx2tok[:10]\n",
    "\n",
    "['>',\n",
    " 'subject:',\n",
    " 'from:',\n",
    " 'lines:',\n",
    " 'organization:',\n",
    " '|',\n",
    " '-',\n",
    " 'would',\n",
    " 're:',\n",
    " '--']\n",
    "\n",
    "> tok2idx['would']\n",
    "\n",
    "7\n",
    " ```"
   ],
   "metadata": {
    "id": "CrtKqVZDJVU5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training a NB Classifier\n",
    "\n",
    "Naive Bayes classifier is a simple conditional probability model based on applying Bayes' theorem with strong feature independence assumption. For more details, you should carefully read the lecture slides.\n",
    "\n",
    "In essense, we need to build a classifier that computes the following:\n",
    "\n",
    "$$argmax_{c\\in C} P(c)\\prod_{w\\in d} P(w|c)$$\n",
    "\n",
    "That is, for each class $c$, we compute the product of the class prior $P(c)$ and the conditional probabilities of words given the class $P(w|c)$ in a document $d$.\n",
    "\n",
    "To do this, we need to estimate the prior class probabilities $P(c)$ and the conditional probabilities $P(w|c)$. We will use the normalized frequencies to estimate these probabilities.\n",
    "\n",
    "For example, $P(c=rec.autos)$ can be estimated by the number of documents that belong to the class divided it by the total number of documents.\n",
    "\n",
    "Likewise, $P(w=car|c=rec.autos)$ can be estimated by the fraction of the word $w$ appears among all words in documents of the class $c$.\n",
    "\n",
    "To handle the zero probability issue, you should also apply the 'add-1' smoothing. See the lecture slides.\n",
    "\n",
    "Now, the following Numpy arrays (i.e, `cond_prob` and `prior_prob`) will contain the estimated probabilities."
   ],
   "metadata": {
    "id": "mQsNTWnPKCAC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "cond_prob = np.zeros((V, C))\n",
    "prior_prob = np.zeros((C))"
   ],
   "metadata": {
    "id": "y05wjdIircPc"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**TODO -- Increment the counts and normalize them properly so that they can be use as the probabilities.**"
   ],
   "metadata": {
    "id": "GRYMTykOPGe4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for d, c in zip(data_tr_prep, data_tr.target):\n",
    "  # TODO -- Complete this for loop block.\n",
    "  for t in d:\n",
    "    if t in tok2idx:\n",
    "      cond_prob[tok2idx[t], c] += 1\n",
    "  prior_prob[c] += 1\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "ebOpqRA7wmTu"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "`prior_prob` should look something like this:\n",
    "\n",
    "```\n",
    "array([0.04242531, 0.05161747, 0.05223617, 0.05214778, 0.05108715,\n",
    "       0.05241294, 0.05170585, 0.05250133, 0.05285487, 0.05276648,\n",
    "       0.05303164, 0.05258971, 0.05223617, 0.05250133, 0.05241294,\n",
    "       0.05294326, 0.04825879, 0.04984974, 0.04109952, 0.03332155])\n",
    "```\n",
    "\n",
    "`cond_prob[10]` should look something like this:\n",
    "\n",
    "```\n",
    "array([0.00802263, 0.00404768, 0.00520794, 0.00410638, 0.00516728,\n",
    "       0.00250812, 0.00143359, 0.0081197 , 0.00944117, 0.00747272,\n",
    "       0.00482113, 0.00474687, 0.0053405 , 0.00616861, 0.00579096,\n",
    "       0.00451822, 0.00591574, 0.00497174, 0.00676319, 0.00629697])\n",
    "```"
   ],
   "metadata": {
    "id": "D_LoH5uhPx3T"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference\n",
    "\n",
    "You will test your classifier with unseen examples (test dataset).\n",
    "\n",
    "**TODO -- Apply `text_prep` on `data_ts` in the same way as you did earlier.**"
   ],
   "metadata": {
    "id": "uT9_ho7dQZEL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "data_ts = fetch_20newsgroups(subset='test', shuffle=True, random_state=42)\n",
    "data_ts_prep = []\n",
    "# TODO -- Apply text_prep on data_ts and fill in data_ts_prep\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ysZJwUe1zkT8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1674530162344,
     "user_tz": 300,
     "elapsed": 4756,
     "user": {
      "displayName": "Jiho Noh",
      "userId": "12416051139264536919"
     }
    },
    "outputId": "e26fec4c-1e77-4af1-fdb0-c57586d61bf4"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, *make a prediction*.\n",
    "\n",
    "For each test document, compute the \"argmax\" formula shown earlier. The argmax should tell you the class that maximizes the product of the prior/conditional probabilities.\n",
    "\n",
    "You should apply log to the product for computational stability and less expansive computation. Computer prefers addition to multiplication.\n"
   ],
   "metadata": {
    "id": "Jy4gbbUXRI8f"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import math\n",
    "math.log(2)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dxIMDb2lnUQc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1675383126674,
     "user_tz": 300,
     "elapsed": 136,
     "user": {
      "displayName": "Jiho Noh",
      "userId": "12416051139264536919"
     }
    },
    "outputId": "933cb61f-c425-4f3e-9b25-11393cca0c4f"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "V=3\n",
    "cond_prob = np.zeros((10, 2))\n",
    "cond_prob = cond_prob + 1\n",
    "cond_prob /= (np.sum(cond_prob, axis=0) + V)\n",
    "cond_prob"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hxMFkOJ2uncI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1675385165954,
     "user_tz": 300,
     "elapsed": 131,
     "user": {
      "displayName": "Jiho Noh",
      "userId": "12416051139264536919"
     }
    },
    "outputId": "76cd9f0a-8795-482d-bb53-672badbab7e4"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "pred = []\n",
    "current_high = 0\n",
    "for d, c in zip(data_ts_prep, data_ts.target):\n",
    "  for c_i in range(len(data_ts.target_names)):\n",
    "    p = math.log(prior_prob[c_i])\n",
    "    for t in d:\n",
    "       p = p + math.log(cond_prob[tok2idx[t], c_i])\n",
    "      #sum up the logprob\n",
    "  # TODO -- implement this for loop to make predictions\n",
    "\n"
   ],
   "metadata": {
    "id": "73CL3Gtezl9t"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Once, you made all the predictions for the testing examples, you can run a evaluation metric for accuracy.\n",
    "\n",
    "If everything is correct, you should get around 70-77% accuracy.\n"
   ],
   "metadata": {
    "id": "QCQuNkPkSxvd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(data_ts.target, pred)\n"
   ],
   "metadata": {
    "id": "3pPcKuHAS4y5"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bonus points\n",
    "\n",
    "If everything is correct up to this point, you will get 100.\n",
    "\n",
    "You can earn more points. You can do further to increase the accuracy score. (**Note, you should not simply use a pre-built ML function from an available library for this additional work. It would be best if you enhanced your model through your own implementation**)\n",
    "\n",
    "**If you can reach 80% or more, you will get additional points.**"
   ],
   "metadata": {
    "id": "_x4kvfxSTT4n"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "WpK56yraeUOr"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
